# hello_pytorch_gpu_apptainer.sub
# Submit file to access the GPU via apptainer

universe = container
container_image = oras://ghcr.io/uw-madison-dsi/pixi-docker-chtc:apptainer-hello-pytorch-noble-cuda-12.9

# set the log, error and output files
log = hello_pytorch_gpu_apptainer.log.txt
error = hello_pytorch_gpu_apptainer.err.txt
output = hello_pytorch_gpu_apptainer.out.txt

# set the executable to run
executable = hello_pytorch_gpu_apptainer.sh
arguments = $(Process)

+JobDurationCategory = "Medium"

# transfer training data files and runtime source files to the compute node
transfer_input_files = MNIST_data.tar.gz,../../src

# transfer the serialized trained model back
transfer_output_files = mnist_cnn.pt

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# We require a machine with a modern version of the CUDA driver
# requirements = (Target.CUDADriverVersion >= 12.0)
# requirements = (HasCHTCStaging == true)
# Don't use CentOS7 to ensure pty support
requirements = (OpSysMajorVer > 7)

# Optional: specify the GPU hardware architecture required
# Check against the CUDA GPU Compute Capability for your software
# e.g. python -c "import torch; print(torch.cuda.get_arch_list())"
# The listed 'sm_xy' values show the x.y gpu capability supported
gpus_minimum_capability = 5.0

# We must request 1 CPU in addition to 1 GPU
request_cpus = 1
request_gpus = 1

# select some memory and disk space
request_memory = 2GB
request_disk = 6GB

# required memory in MB (x000 is xGB)
# gpus_minimum_memory = 6000

# Opt in to using CHTC GPU Lab resources
+WantGPULab = true
# Specify short job type to run more GPUs in parallel
# Can also request "medium" or "long"
+GPUJobLength = "short"

+ProjectName="UWMadison_Feickert"

# Tell HTCondor to run 1 instances of our job:
queue 1
